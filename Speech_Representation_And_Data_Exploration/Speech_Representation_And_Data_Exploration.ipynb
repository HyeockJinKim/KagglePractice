{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Refferenced Kaggle Competition Page https://www.kaggle.com/davids1992/speech-representation-and-data-exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original Page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Hello! This is my very first Kernel. It is meant to give a grasp of a problem of speech representation. I'd also like to take a look on a features specific to this dataset.\n",
    "\n",
    "Content:\n",
    "\n",
    "    1. Visualization of the recordings - input features\n",
    "        1.1. Wave and spectrogram\n",
    "        1.2. MFCC\n",
    "        1.3. Sprectrogram in 3d\n",
    "        1.4. Silence removal\n",
    "        1.5. Resampling - dimensionality reductions\n",
    "        1.6. Features extraction steps\n",
    "    2. Dataset investigation\n",
    "        2.1. Number of files\n",
    "        2.2. Mean spectrograms and fft\n",
    "        2.3. Deeper into recordings\n",
    "        2.4. Length of recordings\n",
    "        2.5. Note on Gaussian Mixtures modeling\n",
    "        2.6. Frequency components across the words\n",
    "        2.7. Anomaly detection\n",
    "    3. Where to look for the inspiration\n",
    "\n",
    "All we need is here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from os.path import isdir, join\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "#Math\n",
    "import numpy as np\n",
    "from scipy.fftpack import fft\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "import librosa\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import IPython.display as ipd\n",
    "import librosa.display\n",
    "\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Visualization\n",
    "\n",
    "There are two theories of a human hearing - place ( https://en.wikipedia.org/wiki/Place_theory_(hearing) (frequency-based) and temporal (https://en.wikipedia.org/wiki/Temporal_theory_(hearing) ) In speech recognition, I see two main tendencies - to input spectrogram (frequencies), and more sophisticated features MFCC - Mel-Frequency Cepstral Coefficients, PLP. You rarely work with raw, temporal data.\n",
    "\n",
    "Let's visualize some recordings!\n",
    "1.1. Wave and spectrogram:\n",
    "\n",
    "Choose and read some file:\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Wave and spectrogram:\n",
    "\n",
    "Choose and read some file: "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
